ğŸ§  Ollama Chatbot â€“ ChatGPT-Style Interface (Mac Local)

Welcome to the AI Chatbot with Memory, a local-first, privacy-friendly chatbot that mimics the ChatGPT experience â€” built with Streamlit, powered by Ollama, and running LLMs directly on your MacBook (M1/M2/M3) using models like phi, llama2, or mistral.

âš¡ï¸ No cloud needed. Fully local. Chat like ChatGPT â€“ but itâ€™s all yours.

â¸»

ğŸš€ Features
	â€¢	ğŸ§© Streamlit-based ChatGPT UI
	â€¢	ğŸ§  Conversational memory (chat history)
	â€¢	ğŸ–¥ï¸ Runs locally via Ollama (optimized for macOS)
	â€¢	âš™ï¸ Plug-and-play with models like phi, mistral, llama2, etc.
	â€¢	ğŸ’¬ Chat input at the bottom like ChatGPT
	â€¢	ğŸ”’ 100% private. No data leaves your machine.

â¸»

ğŸ“¦ Requirements
	â€¢	macOS (M1/M2/M3 preferred)
	â€¢	Python 3.10+
	â€¢	Ollama installed locally
	â€¢	Streamlit 1.28+

â¸»

ğŸ› ï¸ Installation

# Clone the repo
git clone https://github.com/yourusername/ai-chatbot-with-memory.git
cd ai-chatbot-with-memory

# (Optional) Create a virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt



â¸»

âš¡ Running the App

Make sure Ollama is installed and running.

Then launch:

streamlit run streamlit_ollama_chatgpt_style.py

ğŸ“ The app will open at http://localhost:8501

â¸»

ğŸ¤– Switch Models Easily

To use a different Ollama model, update this line in your Python code:

response = ollama.chat(model="phi", messages=history)

Change "phi" to:
	â€¢	"llama2"
	â€¢	"mistral"
	â€¢	"codellama" (for code)
	â€¢	"gemma" (Google)
	â€¢	etc.

â¸»

ğŸ“ Project Structure

ai-chatbot-with-memory/
â”‚
â”œâ”€â”€ streamlit_ollama_chatgpt_style.py   # Main app
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ README.md                           # This file
â””â”€â”€ ... (your project files)



â¸»

ğŸŒŸ Credits

Built with â¤ï¸ by Kurshid Shaik
Powered by Ollama, Streamlit, and your local CPU/GPU.

â¸»

ğŸ“œ License

MIT License â€“ Free to use, modify, and distribute.
